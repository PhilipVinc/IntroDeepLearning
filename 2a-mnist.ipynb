{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6387de74-7f19-446c-9c10-3d201a270a87",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "### Hands-on 2a: MNIST\n",
    "Filippo Vicentini and Giuseppe Carleo\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhilipVinc/IntroDeepLearning/blob/master/2a-mnist.ipynb) \n",
    "\n",
    "\n",
    "The objective of this hands-on is to write and optimise an image-classifier that identifies handwritten digits.\n",
    "\n",
    "We will use for this the [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "![title](images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558598f-2027-45fc-8b92-b8e8ae74215e",
   "metadata": {},
   "source": [
    "## 0 - Install Requirements\n",
    "\n",
    "For this example notebook we will need jax+flax+optax for the machine-learning part.\n",
    "\n",
    "For the dataset, instead, we will be using `tensorflow_datasets`, which is a submodule of `tensorflow` that makes it easy to download and load into memory large datasets (such as MNIST and many others).\n",
    "\n",
    "If you are running notebook locally, you need to also install `tensorflow` to make `tensorflow_datasets` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7895a56-6b30-49bc-b07e-1fe5a6a00b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "#!pip install tensorflow_datasets flax jax optax tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f998ee7-4e9d-4538-b125-5222253ad15b",
   "metadata": {},
   "source": [
    "### 0.1 - Utility Functions\n",
    "\n",
    "Don't look here. There is nothing interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561535f1-4d9a-4908-bbb7-e63c94d22d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions (don't worry. you don't need to understand this one)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_img(img, ax=None, title=None):\n",
    "  \"\"\"Shows a single image.\n",
    "  \n",
    "  Must be stored as a 3d-tensor where the last dimension is 1 channel (greyscale)\n",
    "  \"\"\"\n",
    "  if ax is None:\n",
    "    ax = plt.gca()\n",
    "  ax.imshow(img[..., 0], cmap='gray')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  if title:\n",
    "    ax.set_title(title)\n",
    "\n",
    "def show_img_grid(imgs, titles):\n",
    "  \"\"\"Shows a grid of images.\"\"\"\n",
    "  n = int(np.ceil(len(imgs)**.5))\n",
    "  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n",
    "  for i, (img, title) in enumerate(zip(imgs, titles)):\n",
    "    show_img(img, axs[i // n][i % n], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241cfd9e-6a5e-4257-b3b5-2e7bad914a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5988d-c3c3-43d0-abc8-6ceb1c09054d",
   "metadata": {},
   "source": [
    "## 1 - Setting up the dataset\n",
    "First of all, we need to download the dataset.\n",
    "\n",
    "The MNIST dataset is a standard dataset composed of several 28x28 black/white images representing numbers, and a label corresponding to the number that is represented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e05be1-0fd8-4562-8251-f8caa0576d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/filippovicentini/Documents/pythonenvs/ml_teach-env/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/filippovicentini/Documents/pythonenvs/ml_teach-env/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    }
   ],
   "source": [
    "# We use Tensorflow datasets to download and import data in a simple numpy-tensor format\n",
    "# It's just handy. You could use anything else.\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Specify the dataset we are interested in\n",
    "ds_builder = tfds.builder('mnist')\n",
    "\n",
    "# Download the data\n",
    "ds_builder.download_and_prepare()\n",
    "\n",
    "# Get the whole dataset's train set\n",
    "train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b5869-071c-4431-b830-fd837948577d",
   "metadata": {},
   "source": [
    "The dataset is split into two sub-sets: the training dataset that we will use to 'train' our model, and the 'test' dataset, which the model *never sees* during training, but that we use to check that the model performs well.\n",
    "\n",
    "This is to verify that the model does not simply learn _by heart_ the images in the training dataset, but that it actually _learns_ to generalize and works correctly with images that he did not see before.\n",
    "\n",
    "We can inspect the shape of the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6521ad-6be7-46ab-9cf4-28d24da2d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset keys: dict_keys(['image', 'label'])\n",
      "The training dataset has shape: (60000, 28, 28, 1) and dtype uint8\n",
      "The test     dataset has shape: (10000, 28, 28, 1) and dtype uint8\n",
      "\n",
      "The training labels have shape: (60000,) and dtype int64\n",
      "The test     labels have shape: (10000,) and dtype int64\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset keys:\", train_ds.keys())\n",
    "print(f\"The training dataset has shape: {train_ds['image'].shape} and dtype {train_ds['image'].dtype}\")\n",
    "print(f\"The test     dataset has shape: {test_ds['image'].shape} and dtype {train_ds['image'].dtype}\")\n",
    "print(\"\")\n",
    "print(f\"The training labels have shape: {train_ds['label'].shape} and dtype {train_ds['label'].dtype}\")\n",
    "print(f\"The test     labels have shape: {test_ds['label'].shape} and dtype {test_ds['label'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15665ab5-cb0c-4478-a48e-e0515cbe4fef",
   "metadata": {},
   "source": [
    "We can visualize it to understand it a bit more, using an utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6146cb7-be9d-4206-8b60-ae27699f0444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img_grid(\n",
    "    [train_ds['image'][idx] for idx in range(9)],\n",
    "    [f'label={train_ds[\"label\"][idx]}' for idx in range(9)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d357d877-89e9-41ea-8301-d5fe22693a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean     of the data stored in the images are:  33.318421449829934\n",
      "The variance of the data stored in the images are:  6172.850482291347\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean     of the data stored in the images are: \", np.mean(train_ds['image']))\n",
    "print(\"The variance of the data stored in the images are: \", np.var(train_ds['image']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9f1e8-b449-4589-a826-3dd5771db7ce",
   "metadata": {},
   "source": [
    "We have seen that the data is stored in uint8 (an *unsigned* 8-bit integer which can take values from 0 to 255 ).\n",
    "\n",
    "However it is often preferable when working with Neural Networks to work with floating-point values with values around 0 and variance approximately 1. The reasons are 2:\n",
    "\n",
    " - modern CPUs (and to an extent GPUs) are often faster at working with batches (blocks) of floating-point numbers rather than integers [caveats apply]\n",
    " - Many nonlinear functions used in machine-learning have the nonlinear crossover aroud ~0 or ~1/2, so we want our data to be spread around those values\n",
    " - Most research about how to initialize neural-network layers assumes that the input data has mean 0 and variance 1, so to exploit those results we have to rescale our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83684f21-5c8e-423d-8c61-9973988fc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore... let's convert the data!\n",
    "train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "test_ds['image'] = jnp.float32(test_ds['image']) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a79b56-21ac-49ec-9485-2590a18a2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has shape: (60000, 28, 28, 1) and dtype float32\n",
      "The test     dataset has shape: (10000, 28, 28, 1) and dtype float32\n",
      "The mean     of the data stored in the images are:  0.13066326\n",
      "The variance of the data stored in the images are:  0.09471572\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training dataset has shape: {train_ds['image'].shape} and dtype {train_ds['image'].dtype}\")\n",
    "print(f\"The test     dataset has shape: {test_ds['image'].shape} and dtype {train_ds['image'].dtype}\")\n",
    "\n",
    "print(\"The mean     of the data stored in the images are: \", np.mean(train_ds['image']))\n",
    "print(\"The variance of the data stored in the images are: \", np.var(train_ds['image']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf695fb-0fab-49f3-81fd-9a23fc7ea311",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98e56e14-2a4e-4fad-9d47-809ba4b0a354",
   "metadata": {},
   "source": [
    "## 2 - The model (Neural Network)\n",
    "\n",
    "We want now to define the Model.\n",
    "We will use Flax to do that.\n",
    "\n",
    "We want our network to return a probability distribution for the input to correspond to one of several output labels.\n",
    "\n",
    "e.g: if $x$ is an image, then $f : \\mathbb{R}^{28\\times 28}\\rightarrow \\mathbb{R}^{10}$ and $f^{(i)}(x)$ is the probability that the image $x$ represents a $i\\in[0,9]$\n",
    "\n",
    "To make the output of the network a probability distribution, we can use a softmax function, defined as\n",
    "\n",
    "$$\n",
    "\\sigma_i(x) = \\frac{e^{x_i}}{\\sum_i^K e^{x_i} }  \\text{   for  } i\\in [1,K] \\text{ and } x\\in\\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "We want to use a Feedforward network with 2 Dense Layers, relu-nonlinearity and output softmax using Flax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af5cf83-b3e2-4a1d-b204-73f383952134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# We import flax.linen as nn\n",
    "# The reason is that flax.nn is old and deprecated and will be removed one day\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf8a1ee-844e-4342-8578-6a0df21cc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Flax model must be a class sub-classing nn.Module\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    # We can have some attributes of the Model. \n",
    "    # Those are considered compile-time constants and must be hashable\n",
    "    # They are useful to define some variables that might be changed often\n",
    "    hidden_width : int = 1024\n",
    "    \"\"\"\n",
    "    The width of the hidden dense layers in the neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_classes : int = 10\n",
    "    \"\"\"\n",
    "    Number of output classes for the classifier \n",
    "    \"\"\"\n",
    "    \n",
    "    # The body of the model must be defined using the `@nn.compact` decorator.\n",
    "    # Just think of it as boilerplate, and if you are curious, check out\n",
    "    # Flax documentation\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        This function should evaluate the result of the model for an input image\n",
    "        x or a batch of images x.\n",
    "        \n",
    "        x has shape (28,28,1) or (N, 28, 28, 1)\n",
    "        \"\"\"\n",
    "        # we first ensure a single image is a 4-tensor\n",
    "        x = x.reshape(-1, 28, 28, 1)\n",
    "        # Then flatten the x/y/channels dimensions\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        \n",
    "        # First dense layer with large output\n",
    "        x = nn.Dense(features=self.hidden_width)(x)\n",
    "        \n",
    "        # First nonlinear activation function\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        # Last dense layer with n_classes output\n",
    "        x = nn.Dense(features=self.n_classes)(x)\n",
    "        \n",
    "        # transform the output to be a log-probability distribution\n",
    "        return nn.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1324af-5409-4a2c-9fc3-1ae9e09e009b",
   "metadata": {},
   "source": [
    "Let's initialize the model:\n",
    " \n",
    " - We need a seed for the RNG that generates the initial weights\n",
    " - We need a sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add7d9bd-8028-43e0-b62f-d1c8f05b0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "\n",
    "model = Model(hidden_width = 1024, n_classes=10)\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "sample_input = jnp.ones([1, 28, 28, 1])\n",
    "\n",
    "pars = model.init(key, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604bf44-3b6e-4550-93bf-af5253a50364",
   "metadata": {},
   "source": [
    "we can inspect the parameters `pars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e0b121-050c-443e-ac7f-9c639b2fb4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.07704171, 0.08986449, 0.07054991, 0.04636909, 0.30741608,\n",
       "              0.064256  , 0.10086796, 0.06952595, 0.10017835, 0.07393045]],            dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample application:\n",
    "jnp.exp(model.apply(pars, jnp.ones([1, 28, 28, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebdbb9-8e06-4620-89f8-572f752ff73e",
   "metadata": {},
   "source": [
    "## 3 - Writing the loss function\n",
    "\n",
    "We now want to take as a loss function the distance between the _predicted_ probability given by the model $q_W^{(i)}(x)$ and the actualy probabilith $p^{(i)}(x)$.\n",
    "\n",
    "The actual probability is a delta function: it is zero for every label except for the correct one, for which it is 1.\n",
    "\n",
    "To perform this, we can use one-hot encoding, which takes an integer value in $i\\in[0..K]$ and returns a vector in $R^K$ where only the i-th component is 1 and the other are zero: $v_j = \\delta_{i,j}$.\n",
    "\n",
    "See the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d168c699-dbc5-4f84-9f1f-40511e35050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 becomes: [1. 0. 0. 0. 0.]\n",
      "1 becomes: [0. 1. 0. 0. 0.]\n",
      "2 becomes: [0. 0. 1. 0. 0.]\n",
      "3 becomes: [0. 0. 0. 1. 0.]\n",
      "4 becomes: [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"{i} becomes: {jax.nn.one_hot(i, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72691d-9a4e-4826-a042-67b0aaa3238e",
   "metadata": {},
   "source": [
    "For the loss function, i'll draw from my vast knowledge of loss functions (aka: [here](https://optax.readthedocs.io/en/latest/api.html)) and choose `optax.softmax_cross_entropy`.\n",
    "\n",
    "However, for the sake of completeness, i'll write it down here by hand.\n",
    "\n",
    "`?optax.softmax_cross_entropy`\n",
    "> Computes the softmax cross entropy between sets of logits and labels.\n",
    ">\n",
    ">Measures the probability error in discrete classification tasks in which\n",
    ">the classes are mutually exclusive (each entry is in exactly one class).\n",
    ">For example, each CIFAR-10 image is labeled with one and only one label:\n",
    ">an image can be a dog or a truck, but not both.\n",
    ">\n",
    ">References:\n",
    "> [Goodfellow et al, 2016](http://www.deeplearningbook.org/contents/prob.html)\n",
    ">\n",
    ">Args:\n",
    ">\n",
    ">  logits: unnormalized log probabilities.\n",
    ">\n",
    ">  labels: a valid probability distribution (non-negative, sum to 1), e.g a\n",
    ">    one hot encoding of which class is the correct one for each input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7a0b6-6157-4bf5-a6a0-39888bebe46b",
   "metadata": {},
   "source": [
    "The cross entropy between an approximate distribution $P_W(x)$ and a target distribution $Q(x)$ is defined as \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(W) = - \\langle log P_W(x) \\rangle_{x \\approx Q(x)} = - \\sum_{x \\approx Q(x)} log P_W(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4112499-6b6f-445e-bd18-22fd5b0b2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function that we will use\n",
    "def cross_entropy(*, logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e90668ed-0090-446a-81d3-4f6e1763b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, images, labels):\n",
    "    \"\"\"\n",
    "    Loss function minimised during training of the model.\n",
    "    \"\"\"\n",
    "    # compute the output of the model, which gives the \n",
    "    # log-probability distribution over the possible classes (0...9)\n",
    "    logits = model.apply({'params': params}, images)\n",
    "    # feed it to the cross_entropy\n",
    "    return cross_entropy(logits=logits, labels=labels)\n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    \"\"\"\n",
    "    Compute metrics of the model during training.\n",
    "    \n",
    "    Returns the loss and the accuracy.\n",
    "    \"\"\"\n",
    "    loss = cross_entropy(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ff54a-3d6a-42e7-bdf3-23a9b0d899fd",
   "metadata": {},
   "source": [
    "## 4 - Create the setup and training loop\n",
    "\n",
    "We need to define some functions to create the initial state and we need to define a function to execute one training step, and the whole training loop.\n",
    "\n",
    "For the optimiser, we use optimisers defined in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50082ccb-9022-496a-8898-f2b645116347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "def create_train_state(rng, optimiser):\n",
    "    \"\"\"Creates initial `TrainState`, holding the current parameters, state of the\n",
    "    optimiser and other metadata.\n",
    "    \"\"\"\n",
    "    # Construct the model parameters\n",
    "    params = model.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "        \n",
    "    # Package all those informations in the model state\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=optimiser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d0ec7de-f082-4276-96e6-86c59712ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_metrics(params, batch):\n",
    "    \"\"\"\n",
    "    This function evaluates the metrics without training the model.\n",
    "    \n",
    "    Used to check the performance of the network on training and test datasets.\n",
    "    \"\"\"\n",
    "    logits = model.apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f6b6254-da8a-4bd5-ad56-15b13a3c2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial is handy as it can be used to 'fix' some arguments to a function.\n",
    "# so partial(f, x)(y) == f(x,y)\n",
    "from functools import partial\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"\n",
    "    Train for a single step.\n",
    "    \n",
    "    The input images `batch` should not be too large, otherwise we will run\n",
    "    out of memory. Therefore the input should be 'batched', meaning should be\n",
    "    separated into small blocks of ~hundreds (instead of tens of thousands)\n",
    "    iamges.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fix some arguments to the loss function (so that the only 'free' parameter is\n",
    "    # the parameters of the network.\n",
    "    _loss_fn = partial(loss_fn, images=batch['image'], labels=batch['label'])\n",
    "    \n",
    "    # construct the function returning the loss value and gradient.\n",
    "    val_grad_fn = jax.value_and_grad(_loss_fn)\n",
    "    \n",
    "    # compute loss and gradient\n",
    "    loss, grads = val_grad_fn(state.params)\n",
    "    \n",
    "    # update the state parameters with the new gradients\n",
    "    # objects are immutable so the output of this function is a different\n",
    "    # object than the starting one.\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    # Evaluate the network again to get the log-probability distribution\n",
    "    # over the batch images\n",
    "    metrics = eval_metrics(state.params, batch)\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2f17cf0-198d-4d57-88d9-de60e6e76492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng, *, max_steps=None):\n",
    "    \"\"\"Train for a single `epoch`.\n",
    "    \n",
    "    And epoch is composed of several steps, where every step is taken by updating\n",
    "    the network parameters with a small mini-batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # total number of training images\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    \n",
    "    # Compute how many steps are present in this epoch.\n",
    "    # In one epoch we want to go through the whole dataset.\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    # Truncate the number of steps (used to speed up training)\n",
    "    # Sometimes we might want not to go through the whole dataset\n",
    "    # in an epoch.\n",
    "    if max_steps is not None:\n",
    "        steps_per_epoch = min(steps_per_epoch, max_steps)\n",
    "\n",
    "    # generate a random permutation of the indices to shuffle the training\n",
    "    # dataset, and reshape it to a set of batches.\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    \n",
    "    # execute the training step for every mini-batch\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "            for k in batch_metrics_np[0]}\n",
    "\n",
    "    return state, epoch_metrics_np\n",
    "\n",
    "\n",
    "def evaluate_model(params, test_ds):\n",
    "    \"\"\"\n",
    "    evaluate the performance of the model on the test dataset\n",
    "    \"\"\"\n",
    "    metrics = eval_metrics(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1db647-2349-46d8-8f6f-8bebedf73872",
   "metadata": {},
   "source": [
    "# 5 - Running the optimisation\n",
    "\n",
    "We will now finally run the optimisation.  Below we will define all the required HyperParameters.\n",
    "\n",
    "For the optimiser, we pick one from the [optax](https://optax.readthedocs.io) package, which is very comprehensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1123ddd-9083-4122-b47d-07513797aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of optimiser HyperParameters\n",
    "\n",
    "learning_rate = 0.1\n",
    "\"\"\"\n",
    "Standard SGD step size\n",
    "\"\"\"\n",
    "momentum = 0.9\n",
    "\"\"\"\n",
    "Amount of memntum. The maximum effective learning rate will be\n",
    "$ learning_rate * momentum/(1-momentum)$\n",
    "\"\"\"\n",
    "\n",
    "# Construct the optimiser\n",
    "# we use optimisers from the optax package which is a very comprehensive\n",
    "# optimiser library\n",
    "optimiser = optax.sgd(learning_rate, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae251b-a760-4156-ae09-51e4476d769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 32\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "max_steps = 200\n",
    "\"\"\"\n",
    "Cutoff to the number of steps (minibatches) in an epoch\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7263bc-b1f0-4a71-8b2a-ff19daf8257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the rng to get two keys, one to 'shuffle' the dataset at every iteration,\n",
    "# and one to initialise the network\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a077215-b821-45cf-8747-cd7154f4351a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc85daed3240729af34aeb67aea1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, loss: 0.1508, accuracy: 95.71\n",
      " test epoch: 1, loss: 0.19, accuracy: 94.08\n",
      "train epoch: 2, loss: 0.0690, accuracy: 98.10\n",
      " test epoch: 2, loss: 0.12, accuracy: 96.87\n",
      "train epoch: 3, loss: 0.0500, accuracy: 98.69\n",
      " test epoch: 3, loss: 0.11, accuracy: 96.99\n",
      "train epoch: 4, loss: 0.0358, accuracy: 99.03\n",
      " test epoch: 4, loss: 0.10, accuracy: 97.24\n",
      "train epoch: 5, loss: 0.0272, accuracy: 99.29\n",
      " test epoch: 5, loss: 0.15, accuracy: 96.61\n",
      "train epoch: 6, loss: 0.0248, accuracy: 99.35\n",
      " test epoch: 6, loss: 0.12, accuracy: 97.26\n",
      "train epoch: 7, loss: 0.0184, accuracy: 99.55\n",
      " test epoch: 7, loss: 0.11, accuracy: 97.72\n",
      "train epoch: 8, loss: 0.0169, accuracy: 99.55\n",
      " test epoch: 8, loss: 0.14, accuracy: 97.00\n",
      "train epoch: 9, loss: 0.0144, accuracy: 99.63\n",
      " test epoch: 9, loss: 0.11, accuracy: 97.72\n",
      "train epoch: 10, loss: 0.0115, accuracy: 99.73\n",
      " test epoch: 10, loss: 0.13, accuracy: 97.56\n"
     ]
    }
   ],
   "source": [
    "# Import the TQDM progress bar module using automatic notebook detection\n",
    "# Otherwise it would not work..\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "state = create_train_state(init_rng, optimiser)\n",
    "\n",
    "metrics = {\"test_loss\" : [], \"test_accuracy\": [], \"train_loss\":[], \"train_accuracy\":[]}\n",
    "\n",
    "with tqdm(range(1, num_epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Use a separate PRNG key to permute image data during shuffling\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        # Run an optimization step over a training batch\n",
    "        state, train_metrics = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "        # Evaluate on the test set after each training epoch\n",
    "        test_loss, test_accuracy = evaluate_model(state.params, test_ds)\n",
    "        pbar.write('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, train_metrics['loss'], train_metrics['accuracy'] * 100))\n",
    "        pbar.write(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))\n",
    "\n",
    "        # save data\n",
    "        metrics[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        metrics[\"train_accuracy\"].append(train_metrics[\"accuracy\"])\n",
    "        metrics[\"test_loss\"].append(test_loss)\n",
    "        metrics[\"test_accuracy\"].append(test_accuracy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3489436-f20c-4e1b-8d2c-05ac86769fc0",
   "metadata": {},
   "source": [
    "We now want to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefca0cb-80ed-43df-84de-1b4073a84d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,3))\n",
    "\n",
    "axs[0].plot(metrics[\"train_loss\"], label=\"train\")\n",
    "axs[0].plot(metrics[\"test_loss\"], label=\"test\")\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Epoch #\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "axs[1].plot(metrics[\"train_accuracy\"], label=\"train\")\n",
    "axs[1].plot(metrics[\"test_accuracy\"], label=\"test\")\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Epoch #\")\n",
    "axs[1].set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae58e9-826f-437f-b497-d4174885d461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Teaching)",
   "language": "python",
   "name": "teach-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
